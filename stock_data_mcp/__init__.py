import os
import time
import json
import logging
import akshare as ak
import argparse
import requests
import pandas as pd
from fastmcp import FastMCP
from pydantic import Field
from datetime import datetime, timedelta
from starlette.middleware.cors import CORSMiddleware
from .cache import CacheKey
from .data_provider import (
    DataFetcherManager,
    to_chinese_columns,
    COLUMN_MAPPING_TO_CN,
)

_LOGGER = logging.getLogger(__name__)
_LOGGER.setLevel(logging.INFO)

mcp = FastMCP(name="stock-data-mcp", version="0.2.0")

# å…¨å±€æ•°æ®è·å–ç®¡ç†å™¨ï¼ˆæ”¯æŒå¤šæ•°æ®æºè‡ªåŠ¨æ•…éšœè½¬ç§»ï¼‰
_data_manager = None

def get_data_manager() -> DataFetcherManager:
    """è·å–å…¨å±€æ•°æ®ç®¡ç†å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global _data_manager
    if _data_manager is None:
        _data_manager = DataFetcherManager()
    return _data_manager

field_symbol = Field(description="è‚¡ç¥¨ä»£ç ")
field_market = Field("sh", description="è‚¡ç¥¨å¸‚åœºï¼Œä»…æ”¯æŒ: sh(ä¸Šè¯), sz(æ·±è¯), hk(æ¸¯è‚¡), us(ç¾è‚¡), ä¸æ”¯æŒåŠ å¯†è´§å¸")

OKX_BASE_URL = os.getenv("OKX_BASE_URL") or "https://www.okx.com"
BINANCE_BASE_URL = os.getenv("BINANCE_BASE_URL") or "https://www.binance.com"
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10) AppleWebKit/537.36 Chrome/139"


@mcp.tool(
    title="æŸ¥æ‰¾è‚¡ç¥¨ä»£ç ",
    description="æ ¹æ®è‚¡ç¥¨åç§°ã€å…¬å¸åç§°ç­‰å…³é”®è¯æŸ¥æ‰¾è‚¡ç¥¨ä»£ç , ä¸æ”¯æŒåŠ å¯†è´§å¸ã€‚"
                "è¯¥å·¥å…·æ¯”è¾ƒè€—æ—¶ï¼Œå½“ä½ çŸ¥é“è‚¡ç¥¨ä»£ç æˆ–ç”¨æˆ·å·²æŒ‡å®šè‚¡ç¥¨ä»£ç æ—¶ï¼Œå»ºè®®ç›´æ¥é€šè¿‡è‚¡ç¥¨ä»£ç ä½¿ç”¨å…¶ä»–å·¥å…·",
)
def search(
    keyword: str = Field(description="æœç´¢å…³é”®è¯ï¼Œå…¬å¸åç§°ã€è‚¡ç¥¨åç§°ã€è‚¡ç¥¨ä»£ç ã€è¯åˆ¸ç®€ç§°"),
    market: str = field_market,
):
    info = ak_search(None, keyword, market)
    if info is not None:
        suffix = f"äº¤æ˜“å¸‚åœº: {market}"
        return "\n".join([info.to_string(), suffix])
    return f"Not Found for {keyword}"


@mcp.tool(
    title="è·å–è‚¡ç¥¨ä¿¡æ¯",
    description="æ ¹æ®è‚¡ç¥¨ä»£ç å’Œå¸‚åœºè·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯, ä¸æ”¯æŒåŠ å¯†è´§å¸",
)
def stock_info(
    symbol: str = field_symbol,
    market: str = field_market,
):
    markets = [
        ["sh", ak.stock_individual_info_em],
        ["sz", ak.stock_individual_info_em],
        ["hk", ak.stock_hk_security_profile_em],
    ]
    for m in markets:
        if m[0] != market:
            continue
        all = ak_cache(m[1], symbol=symbol, ttl=43200)
        if all is None or all.empty:
            continue
        return all.to_string()

    info = ak_search(symbol, market)
    if info is not None:
        return info.to_string()
    return f"Not Found for {symbol}.{market}"


@mcp.tool(
    title="è·å–è‚¡ç¥¨å†å²ä»·æ ¼",
    description="æ ¹æ®è‚¡ç¥¨ä»£ç å’Œå¸‚åœºè·å–è‚¡ç¥¨å†å²ä»·æ ¼åŠæŠ€æœ¯æŒ‡æ ‡, ä¸æ”¯æŒåŠ å¯†è´§å¸ã€‚æ”¯æŒå¤šæ•°æ®æºè‡ªåŠ¨æ•…éšœè½¬ç§»ã€‚",
)
def stock_prices(
    symbol: str = field_symbol,
    market: str = field_market,
    period: str = Field("daily", description="å‘¨æœŸï¼Œå¦‚: daily(æ—¥çº¿), weekly(å‘¨çº¿ï¼Œä¸æ”¯æŒç¾è‚¡)"),
    limit: int = Field(30, description="è¿”å›æ•°é‡(int)", strict=False),
):
    # å¯¹äº A è‚¡ï¼Œä¼˜å…ˆä½¿ç”¨å¤šæ•°æ®æºç®¡ç†å™¨
    if market in ("sh", "sz"):
        try:
            manager = get_data_manager()
            df = manager.get_daily_data(symbol, days=limit + 62)
            if df is not None and not df.empty:
                # è½¬æ¢ä¸ºä¸­æ–‡åˆ—å
                df = to_chinese_columns(df)
                # æ·»åŠ æ¢æ‰‹ç‡åˆ—ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
                if "æ¢æ‰‹ç‡" not in df.columns:
                    df["æ¢æ‰‹ç‡"] = None
                # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
                add_technical_indicators(df, df["æ”¶ç›˜"], df["æœ€ä½"], df["æœ€é«˜"])
                columns = [
                    "æ—¥æœŸ", "å¼€ç›˜", "æ”¶ç›˜", "æœ€é«˜", "æœ€ä½", "æˆäº¤é‡", "æ¢æ‰‹ç‡",
                    "MACD", "DIF", "DEA", "KDJ.K", "KDJ.D", "KDJ.J", "RSI", "BOLL.U", "BOLL.M", "BOLL.L",
                ]
                available_cols = [c for c in columns if c in df.columns]
                all_lines = df.to_csv(columns=available_cols, index=False, float_format="%.2f").strip().split("\n")
                return "\n".join([all_lines[0], *all_lines[-limit:]])
        except Exception as e:
            _LOGGER.warning(f"å¤šæ•°æ®æºè·å–å¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰é€»è¾‘: {e}")

    # å›é€€åˆ°åŸæœ‰é€»è¾‘ï¼ˆæ¸¯è‚¡ã€ç¾è‚¡ã€ETF ç­‰ï¼‰
    if period == "weekly":
        delta = {"weeks": limit + 62}
    else:
        delta = {"days": limit + 62}
    start_date = (datetime.now() - timedelta(**delta)).strftime("%Y%m%d")
    markets = [
        ["sh", ak.stock_zh_a_hist, {}],
        ["sz", ak.stock_zh_a_hist, {}],
        ["hk", ak.stock_hk_hist, {}],
        ["us", stock_us_daily, {}],
        ["sh", fund_etf_hist_sina, {"market": "sh"}],
        ["sz", fund_etf_hist_sina, {"market": "sz"}],
    ]
    for m in markets:
        if m[0] != market:
            continue
        kws = {"period": period, "start_date": start_date, **m[2]}
        dfs = ak_cache(m[1], symbol=symbol, ttl=3600, **kws)
        if dfs is None or dfs.empty:
            continue
        add_technical_indicators(dfs, dfs["æ”¶ç›˜"], dfs["æœ€ä½"], dfs["æœ€é«˜"])
        columns = [
            "æ—¥æœŸ", "å¼€ç›˜", "æ”¶ç›˜", "æœ€é«˜", "æœ€ä½", "æˆäº¤é‡", "æ¢æ‰‹ç‡",
            "MACD", "DIF", "DEA", "KDJ.K", "KDJ.D", "KDJ.J", "RSI", "BOLL.U", "BOLL.M", "BOLL.L",
        ]
        all = dfs.to_csv(columns=columns, index=False, float_format="%.2f").strip().split("\n")
        return "\n".join([all[0], *all[-limit:]])
    return f"Not Found for {symbol}.{market}"


def stock_us_daily(symbol, start_date="2025-01-01", period="daily"):
    dfs = ak.stock_us_daily(symbol=symbol)
    if dfs is None or dfs.empty:
        return None
    dfs.rename(columns={"date": "æ—¥æœŸ", "open": "å¼€ç›˜", "close": "æ”¶ç›˜", "high": "æœ€é«˜", "low": "æœ€ä½", "volume": "æˆäº¤é‡"}, inplace=True)
    dfs["æ¢æ‰‹ç‡"] = None
    dfs.index = pd.to_datetime(dfs["æ—¥æœŸ"], errors="coerce")
    return dfs[start_date:"2222-01-01"]

def fund_etf_hist_sina(symbol, market="sh", start_date="2025-01-01", period="daily"):
    dfs = ak.fund_etf_hist_sina(symbol=f"{market}{symbol}")
    if dfs is None or dfs.empty:
        return None
    dfs.rename(columns={"date": "æ—¥æœŸ", "open": "å¼€ç›˜", "close": "æ”¶ç›˜", "high": "æœ€é«˜", "low": "æœ€ä½", "volume": "æˆäº¤é‡"}, inplace=True)
    dfs["æ¢æ‰‹ç‡"] = None
    dfs.index = pd.to_datetime(dfs["æ—¥æœŸ"], errors="coerce")
    return dfs[start_date:"2222-01-01"]


@mcp.tool(
    title="è·å–è‚¡ç¥¨/åŠ å¯†è´§å¸ç›¸å…³æ–°é—»",
    description="æ ¹æ®è‚¡ç¥¨ä»£ç æˆ–åŠ å¯†è´§å¸ç¬¦å·è·å–è¿‘æœŸç›¸å…³æ–°é—»",
)
def stock_news(
    symbol: str = Field(description="è‚¡ç¥¨ä»£ç /åŠ å¯†è´§å¸ç¬¦å·"),
    limit: int = Field(15, description="è¿”å›æ•°é‡(int)", strict=False),
):
    news = list(dict.fromkeys([
        v["æ–°é—»å†…å®¹"]
        for v in ak_cache(stock_news_em, symbol=symbol, ttl=3600).to_dict(orient="records")
        if isinstance(v, dict)
    ]))
    if news:
        return "\n".join(news[0:limit])
    return f"Not Found for {symbol}"

def stock_news_em(symbol, limit=20):
    cbk = "jQuery351013927587392975826_1763361926020"
    resp = requests.get(
        "http://search-api-web.eastmoney.com/search/jsonp",
        headers={
            "User-Agent": USER_AGENT,
            "Referer": f"https://so.eastmoney.com/news/s?keyword={symbol}",
        },
        params={
            "cb": cbk,
            "param": '{"uid":"",'
                     f'"keyword":"{symbol}",'
                     '"type":["cmsArticleWebOld"],"client":"web","clientType":"web","clientVersion":"curr",'
                     '"param":{"cmsArticleWebOld":{"searchScope":"default","sort":"default","pageIndex":1,"pageSize":10,'
                     '"preTag":"<em>","postTag":"</em>"}}}',
        },
    )
    text = resp.text.replace(cbk, "").strip().strip("()")
    data = json.loads(text) or {}
    dfs = pd.DataFrame(data.get("result", {}).get("cmsArticleWebOld") or [])
    dfs.sort_values("date", ascending=False, inplace=True)
    dfs = dfs.head(limit)
    dfs["æ–°é—»å†…å®¹"] = dfs["content"].str.replace(r"</?em>", "", regex=True)
    return dfs


@mcp.tool(
    title="Aè‚¡å…³é”®æŒ‡æ ‡",
    description="è·å–ä¸­å›½Aè‚¡å¸‚åœº(ä¸Šè¯ã€æ·±è¯)çš„è‚¡ç¥¨è´¢åŠ¡æŠ¥å‘Šå…³é”®æŒ‡æ ‡",
)
def stock_indicators_a(
    symbol: str = field_symbol,
):
    dfs = ak_cache(ak.stock_financial_abstract_ths, symbol=symbol)
    keys = dfs.to_csv(index=False, float_format="%.3f").strip().split("\n")
    return "\n".join([keys[0], *keys[-15:]])


@mcp.tool(
    title="æ¸¯è‚¡å…³é”®æŒ‡æ ‡",
    description="è·å–æ¸¯è‚¡å¸‚åœºçš„è‚¡ç¥¨è´¢åŠ¡æŠ¥å‘Šå…³é”®æŒ‡æ ‡",
)
def stock_indicators_hk(
    symbol: str = field_symbol,
):
    dfs = ak_cache(ak.stock_financial_hk_analysis_indicator_em, symbol=symbol, indicator="æŠ¥å‘ŠæœŸ")
    keys = dfs.to_csv(index=False, float_format="%.3f").strip().split("\n")
    return "\n".join(keys[0:15])


@mcp.tool(
    title="ç¾è‚¡å…³é”®æŒ‡æ ‡",
    description="è·å–ç¾è‚¡å¸‚åœºçš„è‚¡ç¥¨è´¢åŠ¡æŠ¥å‘Šå…³é”®æŒ‡æ ‡",
)
def stock_indicators_us(
    symbol: str = field_symbol,
):
    dfs = ak_cache(ak.stock_financial_us_analysis_indicator_em, symbol=symbol, indicator="å•å­£æŠ¥")
    keys = dfs.to_csv(index=False, float_format="%.3f").strip().split("\n")
    return "\n".join(keys[0:15])


@mcp.tool(
    title="è·å–å½“å‰æ—¶é—´åŠAè‚¡äº¤æ˜“æ—¥ä¿¡æ¯",
    description="è·å–å½“å‰ç³»ç»Ÿæ—¶é—´åŠAè‚¡äº¤æ˜“æ—¥ä¿¡æ¯ï¼Œå»ºè®®åœ¨è°ƒç”¨å…¶ä»–éœ€è¦æ—¥æœŸå‚æ•°çš„å·¥å…·å‰ä½¿ç”¨è¯¥å·¥å…·",
)
def get_current_time():
    now = datetime.now()
    week = "æ—¥ä¸€äºŒä¸‰å››äº”å…­æ—¥"[now.isoweekday()]
    texts = [f"å½“å‰æ—¶é—´: {now.isoformat()}, æ˜ŸæœŸ{week}"]
    dfs = ak_cache(ak.tool_trade_date_hist_sina, ttl=43200)
    if dfs is not None:
        start = now.date() - timedelta(days=5)
        ended = now.date() + timedelta(days=5)
        dates = [
            d.strftime("%Y-%m-%d")
            for d in dfs["trade_date"]
            if start <= d <= ended
        ]
        texts.append(f", æœ€è¿‘äº¤æ˜“æ—¥æœ‰: {','.join(dates)}")
    return "".join(texts)

def recent_trade_date():
    now = datetime.now().date()
    dfs = ak_cache(ak.tool_trade_date_hist_sina, ttl=43200)
    if dfs is None:
        return now
    dfs.sort_values("trade_date", ascending=False, inplace=True)
    for d in dfs["trade_date"]:
        if d <= now:
            return d
    return now


@mcp.tool(
    title="Aè‚¡æ¶¨åœè‚¡æ± ",
    description="è·å–ä¸­å›½Aè‚¡å¸‚åœº(ä¸Šè¯ã€æ·±è¯)çš„æ‰€æœ‰æ¶¨åœè‚¡ç¥¨",
)
def stock_zt_pool_em(
    date: str = Field("", description="äº¤æ˜“æ—¥æ—¥æœŸ(å¯é€‰)ï¼Œé»˜è®¤ä¸ºæœ€è¿‘çš„äº¤æ˜“æ—¥ï¼Œæ ¼å¼: 20251231"),
    limit: int = Field(50, description="è¿”å›æ•°é‡(int,30-100)", strict=False),
):
    if not date:
        date = recent_trade_date().strftime("%Y%m%d")
    dfs = ak_cache(ak.stock_zt_pool_em, date=date, ttl=1200)
    cnt = len(dfs)
    try:
        dfs.drop(columns=["åºå·", "æµé€šå¸‚å€¼", "æ€»å¸‚å€¼"], inplace=True)
    except Exception:
        pass
    dfs.sort_values("æˆäº¤é¢", ascending=False, inplace=True)
    dfs = dfs.head(int(limit))
    desc = f"å…±{cnt}åªæ¶¨åœè‚¡\n"
    return desc + dfs.to_csv(index=False, float_format="%.2f").strip()


@mcp.tool(
    title="Aè‚¡å¼ºåŠ¿è‚¡æ± ",
    description="è·å–ä¸­å›½Aè‚¡å¸‚åœº(ä¸Šè¯ã€æ·±è¯)çš„å¼ºåŠ¿è‚¡æ± æ•°æ®",
)
def stock_zt_pool_strong_em(
    date: str = Field("", description="äº¤æ˜“æ—¥æ—¥æœŸ(å¯é€‰)ï¼Œé»˜è®¤ä¸ºæœ€è¿‘çš„äº¤æ˜“æ—¥ï¼Œæ ¼å¼: 20251231"),
    limit: int = Field(50, description="è¿”å›æ•°é‡(int,30-100)", strict=False),
):
    if not date:
        date = recent_trade_date().strftime("%Y%m%d")
    dfs = ak_cache(ak.stock_zt_pool_strong_em, date=date, ttl=1200)
    try:
        dfs.drop(columns=["åºå·", "æµé€šå¸‚å€¼", "æ€»å¸‚å€¼"], inplace=True)
    except Exception:
        pass
    dfs.sort_values("æˆäº¤é¢", ascending=False, inplace=True)
    dfs = dfs.head(int(limit))
    return dfs.to_csv(index=False, float_format="%.2f").strip()


@mcp.tool(
    title="Aè‚¡é¾™è™æ¦œç»Ÿè®¡",
    description="è·å–ä¸­å›½Aè‚¡å¸‚åœº(ä¸Šè¯ã€æ·±è¯)çš„é¾™è™æ¦œä¸ªè‚¡ä¸Šæ¦œç»Ÿè®¡æ•°æ®",
)
def stock_lhb_ggtj_sina(
    days: str = Field("5", description="ç»Ÿè®¡æœ€è¿‘å¤©æ•°ï¼Œä»…æ”¯æŒ: [5/10/30/60]"),
    limit: int = Field(50, description="è¿”å›æ•°é‡(int,30-100)", strict=False),
):
    dfs = ak_cache(ak.stock_lhb_ggtj_sina, symbol=days, ttl=3600)
    dfs = dfs.head(int(limit))
    return dfs.to_csv(index=False, float_format="%.2f").strip()


@mcp.tool(
    title="Aè‚¡æ¿å—èµ„é‡‘æµ",
    description="è·å–ä¸­å›½Aè‚¡å¸‚åœº(ä¸Šè¯ã€æ·±è¯)çš„è¡Œä¸šèµ„é‡‘æµå‘æ•°æ®",
)
def stock_sector_fund_flow_rank(
    days: str = Field("ä»Šæ—¥", description="å¤©æ•°ï¼Œä»…æ”¯æŒ: {'ä»Šæ—¥','5æ—¥','10æ—¥'}ï¼Œå¦‚æœéœ€è¦è·å–ä»Šæ—¥æ•°æ®ï¼Œè¯·ç¡®ä¿æ˜¯äº¤æ˜“æ—¥"),
    cate: str = Field("è¡Œä¸šèµ„é‡‘æµ", description="ä»…æ”¯æŒ: {'è¡Œä¸šèµ„é‡‘æµ','æ¦‚å¿µèµ„é‡‘æµ','åœ°åŸŸèµ„é‡‘æµ'}"),
):
    dfs = ak_cache(ak.stock_sector_fund_flow_rank, indicator=days, sector_type=cate, ttl=1200)
    if dfs is None:
        return "è·å–æ•°æ®å¤±è´¥"
    try:
        dfs.sort_values("ä»Šæ—¥æ¶¨è·Œå¹…", ascending=False, inplace=True)
        dfs.drop(columns=["åºå·"], inplace=True)
    except Exception:
        pass
    try:
        dfs = pd.concat([dfs.head(20), dfs.tail(20)])
        return dfs.to_csv(index=False, float_format="%.2f").strip()
    except Exception as exc:
        return str(exc)


@mcp.tool(
    title="å…¨çƒè´¢ç»å¿«è®¯",
    description="è·å–æœ€æ–°çš„å…¨çƒè´¢ç»å¿«è®¯",
)
def stock_news_global():
    news = []
    try:
        dfs = ak.stock_info_global_sina()
        csv = dfs.to_csv(index=False, float_format="%.2f").strip()
        csv = csv.replace(datetime.now().strftime("%Y-%m-%d "), "")
        news.extend(csv.split("\n"))
    except Exception:
        pass
    news.extend(newsnow_news())
    return "\n".join(news)


def newsnow_news(channels=None):
    base = os.getenv("NEWSNOW_BASE_URL")
    if not base:
        return []
    if not channels:
        channels = os.getenv("NEWSNOW_CHANNELS") or "wallstreetcn-quick,cls-telegraph,jin10"
    if isinstance(channels, str):
        channels = channels.split(",")
    all = []
    try:
        res = requests.post(
            f"{base}/api/s/entire",
            json={"sources": channels},
            headers={
                "User-Agent": USER_AGENT,
                "Referer": base,
            },
            timeout=60,
        )
        lst = res.json() or []
        for item in lst:
            for v in item.get("items", [])[0:15]:
                title = v.get("title", "")
                extra = v.get("extra") or {}
                hover = extra.get("hover") or title
                info = extra.get("info") or ""
                all.append(f"{hover} {info}".strip().replace("\n", " "))
    except Exception:
        pass
    return all


@mcp.tool(
    title="è·å–åŠ å¯†è´§å¸å†å²ä»·æ ¼",
    description="è·å–OKXåŠ å¯†è´§å¸çš„å†å²Kçº¿æ•°æ®ï¼ŒåŒ…æ‹¬ä»·æ ¼ã€äº¤æ˜“é‡å’ŒæŠ€æœ¯æŒ‡æ ‡",
)
def okx_prices(
    instId: str = Field("BTC-USDT", description="äº§å“IDï¼Œæ ¼å¼: BTC-USDT"),
    bar: str = Field("1H", description="Kçº¿æ—¶é—´ç²’åº¦ï¼Œä»…æ”¯æŒ: [1m/3m/5m/15m/30m/1H/2H/4H/6H/12H/1D/2D/3D/1W/1M/3M] é™¤åˆ†é’Ÿä¸ºå°å†™må¤–,å…¶ä½™å‡ä¸ºå¤§å†™"),
    limit: int = Field(100, description="è¿”å›æ•°é‡(int)ï¼Œæœ€å¤§300ï¼Œæœ€å°å»ºè®®30", strict=False),
):
    if not bar.endswith("m"):
        bar = bar.upper()
    res = requests.get(
        f"{OKX_BASE_URL}/api/v5/market/candles",
        params={
            "instId": instId,
            "bar": bar,
            "limit": max(300, limit + 62),
        },
        timeout=20,
    )
    data = res.json() or {}
    dfs = pd.DataFrame(data.get("data", []))
    if dfs.empty:
        return pd.DataFrame()
    dfs.columns = ["æ—¶é—´", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡", "æˆäº¤é¢", "æˆäº¤é¢USDT", "Kçº¿å·²å®Œç»“"]
    dfs.sort_values("æ—¶é—´", inplace=True)
    dfs["æ—¶é—´"] = pd.to_datetime(dfs["æ—¶é—´"], errors="coerce", unit="ms")
    dfs["å¼€ç›˜"] = pd.to_numeric(dfs["å¼€ç›˜"], errors="coerce")
    dfs["æœ€é«˜"] = pd.to_numeric(dfs["æœ€é«˜"], errors="coerce")
    dfs["æœ€ä½"] = pd.to_numeric(dfs["æœ€ä½"], errors="coerce")
    dfs["æ”¶ç›˜"] = pd.to_numeric(dfs["æ”¶ç›˜"], errors="coerce")
    dfs["æˆäº¤é‡"] = pd.to_numeric(dfs["æˆäº¤é‡"], errors="coerce")
    dfs["æˆäº¤é¢"] = pd.to_numeric(dfs["æˆäº¤é¢"], errors="coerce")
    add_technical_indicators(dfs, dfs["æ”¶ç›˜"], dfs["æœ€ä½"], dfs["æœ€é«˜"])
    columns = [
        "æ—¶é—´", "å¼€ç›˜", "æ”¶ç›˜", "æœ€é«˜", "æœ€ä½", "æˆäº¤é‡", "æˆäº¤é¢",
        "MACD", "DIF", "DEA", "KDJ.K", "KDJ.D", "KDJ.J", "RSI", "BOLL.U", "BOLL.M", "BOLL.L",
    ]
    all = dfs.to_csv(columns=columns, index=False, float_format="%.2f").strip().split("\n")
    return "\n".join([all[0], *all[-limit:]])


@mcp.tool(
    title="è·å–åŠ å¯†è´§å¸æ æ†å¤šç©ºæ¯”",
    description="è·å–OKXåŠ å¯†è´§å¸å€Ÿå…¥è®¡ä»·è´§å¸ä¸å€Ÿå…¥äº¤æ˜“è´§å¸çš„ç´¯è®¡æ•°é¢æ¯”å€¼",
)
def okx_loan_ratios(
    symbol: str = Field("BTC", description="å¸ç§ï¼Œæ ¼å¼: BTC æˆ– ETH"),
    period: str = Field("1h", description="æ—¶é—´ç²’åº¦ï¼Œä»…æ”¯æŒ: [5m/1H/1D] æ³¨æ„å¤§å°å†™ï¼Œä»…åˆ†é’Ÿä¸ºå°å†™m"),
):
    res = requests.get(
        f"{OKX_BASE_URL}/api/v5/rubik/stat/margin/loan-ratio",
        params={
            "ccy": symbol,
            "period": period,
        },
        timeout=20,
    )
    data = res.json() or {}
    dfs = pd.DataFrame(data.get("data", []))
    if dfs.empty:
        return pd.DataFrame()
    dfs.columns = ["æ—¶é—´", "å¤šç©ºæ¯”"]
    dfs["æ—¶é—´"] = pd.to_datetime(dfs["æ—¶é—´"], errors="coerce", unit="ms")
    dfs["å¤šç©ºæ¯”"] = pd.to_numeric(dfs["å¤šç©ºæ¯”"], errors="coerce")
    return dfs.to_csv(index=False, float_format="%.2f").strip()


@mcp.tool(
    title="è·å–åŠ å¯†è´§å¸ä¸»åŠ¨ä¹°å–æƒ…å†µ",
    description="è·å–OKXåŠ å¯†è´§å¸ä¸»åŠ¨ä¹°å…¥å’Œå–å‡ºçš„äº¤æ˜“é‡",
)
def okx_taker_volume(
    symbol: str = Field("BTC", description="å¸ç§ï¼Œæ ¼å¼: BTC æˆ– ETH"),
    period: str = Field("1h", description="æ—¶é—´ç²’åº¦ï¼Œä»…æ”¯æŒ: [5m/1H/1D] æ³¨æ„å¤§å°å†™ï¼Œä»…åˆ†é’Ÿä¸ºå°å†™m"),
    instType: str = Field("SPOT", description="äº§å“ç±»å‹ SPOT:ç°è´§ CONTRACTS:è¡ç”Ÿå“"),
):
    res = requests.get(
        f"{OKX_BASE_URL}/api/v5/rubik/stat/taker-volume",
        params={
            "ccy": symbol,
            "period": period,
            "instType": instType,
        },
        timeout=20,
    )
    data = res.json() or {}
    dfs = pd.DataFrame(data.get("data", []))
    if dfs.empty:
        return pd.DataFrame()
    dfs.columns = ["æ—¶é—´", "å–å‡ºé‡", "ä¹°å…¥é‡"]
    dfs["æ—¶é—´"] = pd.to_datetime(dfs["æ—¶é—´"], errors="coerce", unit="ms")
    dfs["å–å‡ºé‡"] = pd.to_numeric(dfs["å–å‡ºé‡"], errors="coerce")
    dfs["ä¹°å…¥é‡"] = pd.to_numeric(dfs["ä¹°å…¥é‡"], errors="coerce")
    return dfs.to_csv(index=False, float_format="%.2f").strip()


@mcp.tool(
    title="è·å–åŠ å¯†è´§å¸åˆ†ææŠ¥å‘Š",
    description="è·å–å¸å®‰å¯¹åŠ å¯†è´§å¸çš„AIåˆ†ææŠ¥å‘Šï¼Œæ­¤å·¥å…·å¯¹åˆ†æåŠ å¯†è´§å¸éå¸¸æœ‰ç”¨ï¼Œæ¨èä½¿ç”¨",
)
def binance_ai_report(
    symbol: str = Field("BTC", description="åŠ å¯†è´§å¸å¸ç§ï¼Œæ ¼å¼: BTC æˆ– ETH"),
):
    res = requests.post(
        f"{BINANCE_BASE_URL}/bapi/bigdata/v3/friendly/bigdata/search/ai-report/report",
        json={
            'lang': 'zh-CN',
            'token': symbol,
            'symbol': f'{symbol}USDT',
            'product': 'web-spot',
            'timestamp': int(time.time() * 1000),
            'translateToken': None,
        },
        headers={
            'User-Agent': USER_AGENT,
            'Referer': f'https://www.binance.com/zh-CN/trade/{symbol}_USDT?type=spot',
            'lang': 'zh-CN',
        },
        timeout=20,
    )
    try:
        resp = res.json() or {}
    except Exception:
        try:
            resp = json.loads(res.text.strip()) or {}
        except Exception:
            return res.text
    data = resp.get('data') or {}
    report = data.get('report') or {}
    translated = report.get('translated') or report.get('original') or {}
    modules = translated.get('modules') or []
    txts = []
    for module in modules:
        if tit := module.get('overview'):
            txts.append(tit)
        for point in module.get('points', []):
            txts.append(point.get('content', ''))
    return '\n'.join(txts)


@mcp.tool(
    title="ç»™å‡ºæŠ•èµ„å»ºè®®",
    description="åŸºäºAIå¯¹å…¶ä»–å·¥å…·æä¾›çš„æ•°æ®åˆ†æç»“æœç»™å‡ºå…·ä½“æŠ•èµ„å»ºè®®",
)
def trading_suggest(
    symbol: str = Field(description="è‚¡ç¥¨ä»£ç æˆ–åŠ å¯†å¸ç§"),
    action: str = Field(description="æ¨èæ“ä½œ: buy/sell/hold"),
    score: int = Field(description="ç½®ä¿¡åº¦ï¼ŒèŒƒå›´: 0-100"),
    reason: str = Field(description="æ¨èç†ç”±"),
):
    return {
        "symbol": symbol,
        "action": action,
        "score": score,
        "reason": reason,
    }


@mcp.tool(
    title="è·å–è‚¡ç¥¨å®æ—¶è¡Œæƒ…",
    description="è·å–Aè‚¡/æ¸¯è‚¡å®æ—¶è¡Œæƒ…æ•°æ®ï¼ŒåŒ…æ‹¬æœ€æ–°ä»·ã€æ¶¨è·Œå¹…ã€æˆäº¤é‡ã€æ¢æ‰‹ç‡ã€å¸‚ç›ˆç‡ç­‰ã€‚æ”¯æŒå¤šæ•°æ®æºè‡ªåŠ¨æ•…éšœè½¬ç§»ã€‚",
)
def stock_realtime(
    symbol: str = field_symbol,
    market: str = Field("sh", description="è‚¡ç¥¨å¸‚åœºï¼Œä»…æ”¯æŒ: sh(ä¸Šè¯), sz(æ·±è¯), hk(æ¸¯è‚¡)"),
):
    try:
        manager = get_data_manager()
        quote = manager.get_realtime_quote(symbol)
        if quote is None:
            return f"Not Found for {symbol}.{market}"

        # æ ¼å¼åŒ–è¾“å‡º
        lines = [
            f"è‚¡ç¥¨ä»£ç : {quote.code}",
            f"è‚¡ç¥¨åç§°: {quote.name or '-'}",
            f"æ•°æ®æ¥æº: {quote.source.value if quote.source else '-'}",
            f"æœ€æ–°ä»·: {quote.price or '-'}",
            f"æ¶¨è·Œå¹…: {quote.change_pct or '-'}%",
            f"æ¶¨è·Œé¢: {quote.change_amount or '-'}",
            f"æˆäº¤é‡: {quote.volume or '-'}",
            f"æˆäº¤é¢: {quote.amount or '-'}",
            f"æ¢æ‰‹ç‡: {quote.turnover_rate or '-'}%",
            f"é‡æ¯”: {quote.volume_ratio or '-'}",
            f"æŒ¯å¹…: {quote.amplitude or '-'}%",
            f"ä»Šå¼€: {quote.open_price or '-'}",
            f"æœ€é«˜: {quote.high or '-'}",
            f"æœ€ä½: {quote.low or '-'}",
            f"æ˜¨æ”¶: {quote.pre_close or '-'}",
            f"å¸‚ç›ˆç‡: {quote.pe_ratio or '-'}",
            f"å¸‚å‡€ç‡: {quote.pb_ratio or '-'}",
            f"æ€»å¸‚å€¼: {quote.total_mv or '-'}",
            f"æµé€šå¸‚å€¼: {quote.circ_mv or '-'}",
        ]
        return "\n".join(lines)
    except Exception as e:
        _LOGGER.warning(f"è·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
        return f"è·å– {symbol} å®æ—¶è¡Œæƒ…å¤±è´¥: {e}"


@mcp.tool(
    title="è·å–ç­¹ç åˆ†å¸ƒ",
    description="è·å–Aè‚¡ç­¹ç åˆ†å¸ƒæ•°æ®ï¼ŒåŒ…æ‹¬è·åˆ©æ¯”ä¾‹ã€å¹³å‡æˆæœ¬ã€æˆæœ¬åŒºé—´ã€ç­¹ç é›†ä¸­åº¦ç­‰ã€‚",
)
def stock_chip(
    symbol: str = field_symbol,
):
    try:
        manager = get_data_manager()
        chip = manager.get_chip_distribution(symbol)
        if chip is None:
            return f"Not Found for {symbol}"

        # æ ¼å¼åŒ–è¾“å‡º
        lines = [
            f"è‚¡ç¥¨ä»£ç : {chip.code}",
            f"æ—¥æœŸ: {chip.date or '-'}",
            f"è·åˆ©æ¯”ä¾‹: {chip.profit_ratio or '-'}%",
            f"å¹³å‡æˆæœ¬: {chip.avg_cost or '-'}",
            f"90%æˆæœ¬åŒºé—´: {chip.cost_90_low or '-'} - {chip.cost_90_high or '-'}",
            f"90%é›†ä¸­åº¦: {chip.concentration_90 or '-'}%",
            f"70%æˆæœ¬åŒºé—´: {chip.cost_70_low or '-'} - {chip.cost_70_high or '-'}",
            f"70%é›†ä¸­åº¦: {chip.concentration_70 or '-'}%",
        ]

        # æ·»åŠ ç­¹ç çŠ¶æ€åˆ†æ
        status = chip.get_chip_status()
        if 'chip_level' in status:
            lines.append(f"ç­¹ç çŠ¶æ€: {status['chip_level']}")

        return "\n".join(lines)
    except Exception as e:
        _LOGGER.warning(f"è·å–ç­¹ç åˆ†å¸ƒå¤±è´¥: {e}")
        return f"è·å– {symbol} ç­¹ç åˆ†å¸ƒå¤±è´¥: {e}"


@mcp.tool(
    title="æ‰¹é‡è·å–å®æ—¶è¡Œæƒ…",
    description="æ‰¹é‡è·å–å¤šåªAè‚¡å®æ—¶è¡Œæƒ…æ•°æ®ã€‚æ”¯æŒå¤šæ•°æ®æºè‡ªåŠ¨æ•…éšœè½¬ç§»ã€‚",
)
def stock_batch_realtime(
    symbols: str = Field(description="è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¦‚: 600519,000858,601318"),
    limit: int = Field(20, description="è¿”å›æ•°é‡(int)", strict=False),
):
    try:
        codes = [s.strip() for s in symbols.split(",") if s.strip()]
        if not codes:
            return "è¯·æä¾›æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç "

        codes = codes[:limit]  # é™åˆ¶æ•°é‡
        manager = get_data_manager()
        quotes = manager.prefetch_realtime_quotes(codes)

        if not quotes:
            return "æœªè·å–åˆ°ä»»ä½•è¡Œæƒ…æ•°æ®"

        # è½¬æ¢ä¸º DataFrame è¾“å‡º
        rows = []
        for code, quote in quotes.items():
            rows.append({
                "ä»£ç ": quote.code,
                "åç§°": quote.name or "-",
                "æœ€æ–°ä»·": quote.price,
                "æ¶¨è·Œå¹…": quote.change_pct,
                "æˆäº¤é‡": quote.volume,
                "æˆäº¤é¢": quote.amount,
                "æ¢æ‰‹ç‡": quote.turnover_rate,
                "å¸‚ç›ˆç‡": quote.pe_ratio,
                "å¸‚å‡€ç‡": quote.pb_ratio,
            })

        df = pd.DataFrame(rows)
        return df.to_csv(index=False, float_format="%.2f").strip()
    except Exception as e:
        _LOGGER.warning(f"æ‰¹é‡è·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
        return f"æ‰¹é‡è·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}"


@mcp.tool(
    title="æŸ¥çœ‹æ•°æ®æºçŠ¶æ€",
    description="æŸ¥çœ‹å¤šæ•°æ®æºçš„çŠ¶æ€å’Œç†”æ–­å™¨ä¿¡æ¯",
)
def data_source_status():
    try:
        manager = get_data_manager()
        status = manager.get_status()

        lines = ["=== æ•°æ®æºçŠ¶æ€ ==="]
        for fetcher in status.get('fetchers', []):
            available = "âœ…" if fetcher['available'] else "âŒ"
            lines.append(f"{available} {fetcher['name']} (ä¼˜å…ˆçº§: {fetcher['priority']})")

        lines.append("\n=== ç†”æ–­å™¨çŠ¶æ€ ===")

        for name, breaker_status in [
            ("æ—¥çº¿æ•°æ®", status.get('daily_circuit_breaker', {})),
            ("å®æ—¶è¡Œæƒ…", status.get('realtime_circuit_breaker', {})),
            ("ç­¹ç åˆ†å¸ƒ", status.get('chip_circuit_breaker', {})),
        ]:
            if breaker_status:
                lines.append(f"\n{name}:")
                for source, state in breaker_status.items():
                    state_icon = "ğŸŸ¢" if state['state'] == 'closed' else "ğŸ”´"
                    lines.append(f"  {state_icon} {source}: {state['state']} (å¤±è´¥æ¬¡æ•°: {state['failure_count']})")
            else:
                lines.append(f"\n{name}: æ— ç†”æ–­è®°å½•")

        return "\n".join(lines)
    except Exception as e:
        return f"è·å–æ•°æ®æºçŠ¶æ€å¤±è´¥: {e}"


def ak_search(symbol=None, keyword=None, market=None):
    markets = [
        ["sh", ak.stock_info_a_code_name, "code", "name"],
        ["sh", ak.stock_info_sh_name_code, "è¯åˆ¸ä»£ç ", "è¯åˆ¸ç®€ç§°"],
        ["sz", ak.stock_info_sz_name_code, "Aè‚¡ä»£ç ", "Aè‚¡ç®€ç§°"],
        ["hk", ak.stock_hk_spot, "ä»£ç ", "ä¸­æ–‡åç§°"],
        ["hk", ak.stock_hk_spot_em, "ä»£ç ", "åç§°"],
        ["us", ak.get_us_stock_name, "symbol", "cname"],
        ["us", ak.get_us_stock_name, "symbol", "name"],
        ["sh", ak.fund_etf_spot_ths, "åŸºé‡‘ä»£ç ", "åŸºé‡‘åç§°"],
        ["sz", ak.fund_etf_spot_ths, "åŸºé‡‘ä»£ç ", "åŸºé‡‘åç§°"],
        ["sh", ak.fund_info_index_em, "åŸºé‡‘ä»£ç ", "åŸºé‡‘åç§°"],
        ["sz", ak.fund_info_index_em, "åŸºé‡‘ä»£ç ", "åŸºé‡‘åç§°"],
        ["sh", ak.fund_etf_spot_em, "ä»£ç ", "åç§°"],
        ["sz", ak.fund_etf_spot_em, "ä»£ç ", "åç§°"],
    ]
    for m in markets:
        if market and market != m[0]:
            continue
        all = ak_cache(m[1], ttl=86400, ttl2=86400*7)
        if all is None or all.empty:
            continue
        for _, v in all.iterrows():
            code, name = str(v[m[2]]).upper(), str(v[m[3]]).upper()
            if symbol and symbol.upper() == code:
                return v
            if keyword and keyword.upper() in [code, name]:
                return v
        for _, v in all.iterrows() if keyword else []:
            name = str(v[m[3]])
            if len(keyword) >= 4 and keyword in name:
                return v
            if name.startswith(keyword):
                return v
    return None


def ak_cache(fun, *args, **kwargs) -> pd.DataFrame | None:
    key = kwargs.pop("key", None)
    if not key:
        key = f"{fun.__name__}-{args}-{kwargs}"
    ttl1 = kwargs.pop("ttl", 86400)
    ttl2 = kwargs.pop("ttl2", None)
    cache = CacheKey.init(key, ttl1, ttl2)
    all = cache.get()
    if all is None:
        try:
            _LOGGER.info("Request akshare: %s", [key, args, kwargs])
            all = fun(*args, **kwargs)
            cache.set(all)
        except Exception as exc:
            _LOGGER.exception(str(exc))
    return all

def add_technical_indicators(df, clos, lows, high):
    # è®¡ç®—MACDæŒ‡æ ‡
    ema12 = clos.ewm(span=12, adjust=False).mean()
    ema26 = clos.ewm(span=26, adjust=False).mean()
    df["DIF"] = ema12 - ema26
    df["DEA"] = df["DIF"].ewm(span=9, adjust=False).mean()
    df["MACD"] = (df["DIF"] - df["DEA"]) * 2

    # è®¡ç®—KDJæŒ‡æ ‡
    low_min  = lows.rolling(window=9, min_periods=1).min()
    high_max = high.rolling(window=9, min_periods=1).max()
    rsv = (clos - low_min) / (high_max - low_min) * 100
    df["KDJ.K"] = rsv.ewm(com=2, adjust=False).mean()
    df["KDJ.D"] = df["KDJ.K"].ewm(com=2, adjust=False).mean()
    df["KDJ.J"] = 3 * df["KDJ.K"] - 2 * df["KDJ.D"]

    # è®¡ç®—RSIæŒ‡æ ‡
    delta = clos.diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.rolling(window=14).mean()
    avg_loss = loss.rolling(window=14).mean()
    rs = avg_gain / avg_loss
    df["RSI"] = 100 - (100 / (1 + rs))

    # è®¡ç®—å¸ƒæ—å¸¦æŒ‡æ ‡
    df["BOLL.M"] = clos.rolling(window=20).mean()
    std = clos.rolling(window=20).std()
    df["BOLL.U"] = df["BOLL.M"] + 2 * std
    df["BOLL.L"] = df["BOLL.M"] - 2 * std


def main():
    port = int(os.getenv("PORT", 0)) or 80
    parser = argparse.ArgumentParser(description="Stock Data MCP Server")
    parser.add_argument("--http", action="store_true", help="Use streamable HTTP mode instead of stdio")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to (default: 0.0.0.0)")
    parser.add_argument("--port", type=int, default=port, help=f"Port to listen on (default: {port})")

    args = parser.parse_args()
    mode = os.getenv("TRANSPORT") or ("http" if args.http else None)
    if mode in ["http", "sse"]:
        app = mcp.http_app(transport=mode)
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["GET", "POST", "OPTIONS"],
            allow_headers=["*"],
            expose_headers=["mcp-session-id", "mcp-protocol-version"],
            max_age=86400,
        )
        mcp.run(transport=mode, host=args.host, port=args.port)
    else:
        mcp.run()

if __name__ == "__main__":
    main()
